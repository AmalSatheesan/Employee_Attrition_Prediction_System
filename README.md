# Employee Attrition Prediction System

## ðŸ“Œ Project Overview
This project is an end-to-end Machine Learning system developed to predict whether an employee is likely to leave an organization (Attrition: Yes/No) using historical HR data.

The project follows an industry-aligned ML workflow covering data analysis, preprocessing, feature selection, model training, evaluation, and deployment readiness through a Streamlit application.

---

## ðŸŽ¯ Project Objective
- Predict employee attrition using supervised machine learning
- Identify key factors influencing employee turnover
- Assist HR teams in proactive retention planning
- Build a reproducible and deployment-ready ML solution

---

## ðŸ“Š Dataset Information

- **Dataset Name:** Employee Attrition Dataset  
- **Source:** Kaggle  
- **Link:**  
  ðŸ‘‰ https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset  

- **Total Records (after merge):** ~74,000  
- **Problem Type:** Binary Classification  
- **Target Variable:** `Attrition`  
  - 0 â†’ Stayed  
  - 1 â†’ Left  

### Dataset Setup (Important)
The Kaggle dataset is provided as two files:
- `train.csv`
- `test.csv`

In this project:
- Both files are **merged into a single dataset**
- The merged dataset is then split again into train/test sets during modeling
- This approach ensures a unified EDA and consistent preprocessing pipeline

---

## ðŸ› ï¸ Tech Stack
- **Language:** Python  
- **Data Analysis:** Pandas, NumPy  
- **Visualization:** Matplotlib, Seaborn  
- **Machine Learning:** Scikit-learn, Imbalanced-learn  
- **Model Persistence:** Joblib  
- **Web Application:** Streamlit  
- **Version Control:** Git & GitHub  

---

## ðŸ” Project Workflow (As Implemented in Notebook)

### 1ï¸âƒ£ Data Loading & Initial Inspection
- Load and merge `train.csv` and `test.csv`
- Check shape, data types, missing values
- Analyze class distribution of Attrition

---

### 2ï¸âƒ£ Exploratory Data Analysis (EDA)
- Attrition distribution analysis
- Attrition vs:
  - Age
  - Monthly Income
  - Job Role
  - Years at Company
  - Job Satisfaction
- Distribution plots for numerical features
- Outlier detection using boxplots
- Correlation analysis

Key insights were documented directly in markdown cells.

---

### 3ï¸âƒ£ Data Preprocessing
- Standardization of column names
- Encoding of categorical variables
  - Binary encoding
  - One-hot encoding
- Outlier handling using IQR-based capping
- Feature scaling using `StandardScaler`

---

### 4ï¸âƒ£ Feature Selection
- Chi-Square test for categorical feature relevance
- Correlation-based feature elimination
- Random Forest feature importance analysis
- Removal of identifier leakage (`Employee ID`)

Only meaningful and business-relevant predictors were retained.

---

### 5ï¸âƒ£ Class Imbalance Handling
- Baseline modeling without resampling
- SMOTE applied **only on training data**
- Performance comparison before and after resampling

Recall for attrition cases was prioritized due to business impact.

---

### 6ï¸âƒ£ Model Building & Evaluation
Models trained and compared:
- Logistic Regression
- Random Forest
- Random Forest + SMOTE
- Tuned Random Forest (GridSearchCV)

Evaluation metrics:
- Accuracy
- Precision
- Recall
- F1-score
- Confusion Matrix

---

## ðŸ“ˆ Model Performance Summary
- **Accuracy:** ~75%
- **Recall (Attrition):** ~0.75

> Multiple models converged to similar performance, indicating a realistic upper bound given the available features.  
> Recall was prioritized as missing potential employee exits is more costly than false positives.

---

## ðŸš€ Streamlit Application
A Streamlit-based web application allows users to:
- Input employee details
- Predict attrition (Yes / No)
- View probability scores

### Run the App Locally
```bash
pip install -r requirements.txt
streamlit run app.py

## ðŸ“¦ Model Artifacts (Important Note)

Trained machine learning model files (`.pkl`) are intentionally **not included** in this GitHub repository due to GitHubâ€™s file size limitations.

### ðŸ”¹ Model Handling Strategy
- The final model is trained and serialized within the Jupyter Notebook.
- Model artifacts are excluded from version control to keep the repository lightweight and maintainable.
- The trained model can be:
  - Regenerated by executing the training notebook end-to-end, or
  - Stored externally (e.g., cloud storage, MLflow, or artifact repositories) for deployment and production use.

This approach aligns with **industry best practices** for managing large machine learning artifacts and separating source code from model binaries.

